# coding=utf-8

import argparse
import json
import unicodedata
from abc import ABC
from collections import deque, defaultdict
from copy import deepcopy
from glob import glob
from itertools import groupby
from os import path
from string import punctuation, ascii_letters, digits
from typing import Dict, List, Generator, Tuple, Any

import numpy as np
from logzero import logger
from mojimoji import han_to_zen
from tqdm import tqdm

from instances import (
    ChunkInstance,
    ClozeNotMaskedInstance,
    PzeroMaskedInstance,
    NTCPasDocument,
    Pas,
    PasGoldLabel,
    PasEvalInfo,
    AsTrainingInstance,
    AsGoldPositions,
    AsGoldExo,
    AsPzeroTrainingInstance,
    TARGET_CASES,
    CASE_SURFACES,
    CASE_TYPES,
    NO_ANSWERS,
    EXO_INDEX,
)
from preprocess_ntc import create_corpus, create_ntc_pas_document, print_stats
from tokenizer import load_tokenizer
from utils import read_lines


class Preprocessor(ABC):
    """Preprocess a corpus for each task"""

    def __init__(self):
        self.tokenizer = load_tokenizer()

    def _subwords(self, word: str) -> List[str]:
        """word -> list of subwords

        Args:
            * word (string) : A word to convert to subwords
        Returns:
            * subwords (List[str]) : List of subwords
        """

        normalize_word = unicodedata.normalize('NFKC', word)
        subwords = self.tokenizer.tokenize_into_subwords(normalize_word)

        return subwords


# For Pretraining
def convert_han_to_zen(file_path: str) -> Generator[str, None, None]:
    """This function **MUST BE USED** before processing with the function 'preprocess_with_cabocha.sh.'

    Args:
        * file_path (str): Path to the raw corpus
    Yields:
        * line (str): fill-width converted text
    """
    for line in tqdm(read_lines(file_path)):
        line = han_to_zen(line)

        yield line


def extract_chunk_from_parsed_text(file_path: str) -> Generator[List[ChunkInstance], None, None]:
    """This function converts text parsed by CaboCha into structured data.

    Args:
        * file_path (str):
            Path to the text parsed with Japanese Dependency Parser 'CaboCha' (Kudo and Matsumoto, 2002).
            This text can be generated by the script 'preprocess_with_cabocha.sh'.
    Yields:
        * chunk_instances (List[ChunkInstance]):
            The list of 'ChunkInstance', corresponding to a single sentence.
    """

    idx, head = None, None

    for is_eos, section in groupby(read_lines(file_path), key=lambda x: x.strip() == 'EOS'):
        # if there is more than one 'EOS' line, it indicates a break in the document
        if is_eos:
            n_eos = len(list(section))
            if n_eos > 1:
                yield []
            continue

        a_sentence: List[ChunkInstance] = []
        for is_separation, chunk in groupby(section, key=lambda x: x.startswith('*')):
            if is_separation:
                chunk = list(chunk)
                assert len(chunk) == 1
                _, idx, head, *_ = chunk[0].split()
            else:
                surfs = []
                poss = []
                pos_details = []
                for morph in chunk:
                    surface, word_info, *_ = morph.split('\t')
                    pos, pos_detail, *_ = word_info.split(',')
                    if pos_detail == '空白':
                        continue
                    surfs.append(surface)
                    poss.append(pos)
                    pos_details.append(pos_detail)

                assert len(surfs) == len(poss) == len(pos_details)
                assert idx is not None and head is not None

                chunk_instance = ChunkInstance(
                    idx=idx,
                    head=head,
                    surfs=surfs,
                    poss=poss,
                    pos_details=pos_details
                )
                a_sentence.append(chunk_instance)
                idx, head = None, None

        yield a_sentence

    yield []


class PreprocessForPretrainingCloze(Preprocessor):
    """Preprocessing a raw corpus for Cloze Task"""

    def __init__(self, is_chunk: bool = False) -> None:
        """
        Args:
            * is_chunk (bool):
                If 'file_path' indicates the path to the output file generated by
                the function 'preprocess.extract_chunk_from_parsed_text',
                this value should be True. Defaults to False.
        """
        super().__init__()
        self.is_chunk = is_chunk

    def __call__(self, file_path: str) -> Generator[ClozeNotMaskedInstance, None, None]:
        """text parsed with 'CaboCha' -> instances for cloze task
           or
           jsonl format data generated by 'preprocess.extract_chunk_from_parsed_text' -> instances for cloze task

        Args:
            * file_path (str):
                Path to the text parsed with CaboCha (e.g. 'tests/samples/raw.parsed.txt')
                or jsonl format data generated by the function 'preprocess.extract_chunk_from_parsed_text'
                (e.g. 'tests/samples/raw.chunk.txt')
        Yields:
            * cloze_instance (ClozeNotMaskedInstance):
                The list of instances of cloze task
        """

        # chunks -> instances
        if self.is_chunk:
            for words in self._read_chunk_file(file_path):
                for cloze_instance in self.create_cloze_instances(words):
                    yield cloze_instance

        # parsed text -> chunks -> instances
        else:
            for words in self._read_parsed_file(file_path):
                for cloze_instance in self.create_cloze_instances(words):
                    yield cloze_instance

    def create_cloze_instances(self, words: List[str]) -> Generator[ClozeNotMaskedInstance, None, None]:
        """words -> instances for cloze task

        Args:
            * words (List[str]): The list of words corresponding to a document.
        Yields:
            * ClozeNotMaskedInstance: an instance of cloze task
        """

        subwords: List[str] = []
        for word in words:
            for subword in self._subwords(word):
                subwords.append(subword)
        doc_embed_ids = self.tokenizer.convert_tokens_to_ids(subwords)
        assert isinstance(doc_embed_ids, list)

        model_max_length = self.tokenizer.model_max_length
        n_document_tokens = len(doc_embed_ids)
        n_interval = model_max_length - 2
        for i in range(0, n_document_tokens, n_interval):
            input_embed_ids = self.tokenizer.build_inputs_with_special_tokens(
                doc_embed_ids[i: i + n_interval]
            )
            assert len(input_embed_ids) <= model_max_length

            cloze_instance = ClozeNotMaskedInstance(input_ids=input_embed_ids)
            yield cloze_instance

    @staticmethod
    def _read_parsed_file(file_path: str) -> Generator[List[str], None, None]:
        """text parsed with CaboCha -> words corresponding to a document

        Args:
            * file_path (str): Path to the text parsed with CaboCha
        Yields:
            * words ([List[str]): the list of words corresponding to a document
        """

        words: List[str] = []
        for chunks in extract_chunk_from_parsed_text(file_path):
            if len(chunks) == 0:
                yield words
                words = []
            else:
                for chunk in chunks:
                    words.extend(chunk['surfs'])

        if len(words) != 0:
            yield words

    @staticmethod
    def _read_chunk_file(file_path: str) -> Generator[List[str], None, None]:
        """jsonl format data -> words corresponding to a document

        Args:
            * file_path (str):
                Path to the jsonl format data generated by
                the function 'preprocess.extract_chunk_from_parsed_text'
        Yields:
            * words ([List[str]):
                the list of words corresponding to a document
        """

        loader: Generator[str, None, None] = read_lines(file_path)
        key_func = (lambda x: not bool(x.strip()))

        for is_blank, dumped_lines in tqdm(groupby(loader, key=key_func)):
            if is_blank:
                continue

            words: List[str] = []
            for dumped_line in dumped_lines:
                chunks: List[ChunkInstance] = json.loads(dumped_line)
                for chunk in chunks:
                    words.extend(chunk['surfs'])

            yield words


# --- Pzero Task ---
class PreprocessForPretrainingPzero(Preprocessor):
    """Preprocessing a raw corpus for Pzero Task"""

    def __init__(self, is_chunk: bool = False, max_n_sentences: int = 4) -> None:
        """
        Args:
            * is_chunk (bool):
                If 'file_path' indicates the path to the output file generated by
                the function 'preprocess.extract_chunk_from_parsed_text',
                this value should be True. Defaults to False.
            * max_n_sentences (int, optional):
                Maximum number of sentences used to create an instance. Defaults to 4.
        """

        super().__init__()
        self.is_chunk = is_chunk
        self.max_n_sentences = max_n_sentences

        self.eng_punc_num_chars = han_to_zen(punctuation) + han_to_zen(ascii_letters) + han_to_zen(digits)
        self.exclude_words = {'もの', 'こと', 'ため', '何', '誰'}

    def __call__(self, file_path: str) -> Generator[PzeroMaskedInstance, None, None]:
        """text parsed with CaboCha -> instances for Pzero task
           or
           jsonl format data generated by 'preprocess.extract_chunk_from_parsed_text' -> instances for Pzero task

        Args:
            * file_path (str):
                Path to the text parsed with CaboCha (e.g. 'tests/samples/raw.parsed.txt')
                or jsonl format data generated by the function 'preprocess.extract_chunk_from_parsed_text'
                (e.g. 'tests/samples/raw.chunk.txt')
        Yields:
            * pzero_instance (List[PzeroMaskedInstance]):
                The list of instances of pzero task
        """

        # chunk -> instances
        if self.is_chunk:
            for document in self._read_chunk_file(file_path):
                for pzero_instance in self.create_pzero_instances(document):
                    yield pzero_instance

        # parsed text -> chunk -> instances
        else:
            for document in self._read_parsed_file(file_path):
                for pzero_instance in self.create_pzero_instances(document):
                    yield pzero_instance

    def create_pzero_instances(self, document: List[List[ChunkInstance]]) -> Generator[PzeroMaskedInstance, None, None]:
        """a document -> instance for Pzero task

        Args:
            * document (List[List[ChunkInstance]]): The list of chunks corresponding to a document.
        Yields:
            * pzero_instance (PzeroMaskedInstance): the instance for Pzero task
        """
        current_noun_phrase_idx = 1
        deque_subwords = deque(maxlen=self.max_n_sentences)
        deque_noun_phrase_ids = deque(maxlen=self.max_n_sentences)
        same_noun_phrase_ids: Dict[str, List[int]] = defaultdict(list)
        idx2noun_phrase: Dict[int, str] = dict()

        # document -> sentence
        for chunks in document:  # 'chunks' corresponds to a sentence
            sent_subwords: List[str] = []
            sent_noun_phrase_ids: List[int] = []

            # sentence -> chunk
            for chunk in chunks:
                # extract a noun phrase from the chunk
                noun_phrase, noun_phrase_ids = self._create_noun_phrase_positions(chunk, current_noun_phrase_idx)
                if noun_phrase:
                    same_noun_phrase_ids[noun_phrase].append(current_noun_phrase_idx)
                    idx2noun_phrase[current_noun_phrase_idx] = noun_phrase
                    current_noun_phrase_idx += 1
                assert len(chunk['surfs']) == len(noun_phrase_ids)

                # chunk -> word
                for surf, noun_phrase_idx in zip(chunk['surfs'], noun_phrase_ids):

                    # word -> subword
                    for subword in self._subwords(surf):
                        sent_subwords.append(subword)
                        sent_noun_phrase_ids.append(noun_phrase_idx)

            # append a sentence to deque
            assert len(sent_subwords) == len(sent_noun_phrase_ids)
            deque_subwords.append(sent_subwords)
            deque_noun_phrase_ids.append(sent_noun_phrase_ids)

            # remove the head tokens to fit in the maximum input length of the model
            deque_adjusted_len_subwords = self._reduce_previous_tokens(deque_subwords)
            deque_adjusted_len_noun_phrase_ids = self._reduce_previous_tokens(deque_noun_phrase_ids)
            assert len(deque_adjusted_len_subwords) == len(deque_adjusted_len_noun_phrase_ids)
            assert len(deque_adjusted_len_subwords[0]) == len(deque_adjusted_len_noun_phrase_ids[0])

            # insert sep tokens
            agg_subwords: List[str] = self._insert_sep(deque_adjusted_len_subwords, self.tokenizer.sep_token)
            agg_noun_phrase_ids: List[int] = self._insert_sep(deque_adjusted_len_noun_phrase_ids, 0)
            assert len(agg_subwords) == len(agg_noun_phrase_ids)

            # Create instances
            exist_noun_phrase_ids = set(agg_noun_phrase_ids)
            last_sentence_uniq_noun_phrase_ids = list(
                set(
                    idx for idx in deque_noun_phrase_ids[-1]
                    if idx in exist_noun_phrase_ids and idx != 0
                )
            )
            full_embed_ids = np.array(self.tokenizer.convert_tokens_to_ids(agg_subwords))
            full_noun_phrase_ids = np.array(agg_noun_phrase_ids)

            for target_noun_phrase_idx in last_sentence_uniq_noun_phrase_ids:
                # replacing a noun phrase with a mask
                masked_ids = np.where(full_noun_phrase_ids == target_noun_phrase_idx)[0]
                embed_ids = self._replace_mask(full_embed_ids, masked_ids, self.tokenizer.mask_token_id)
                removed_noun_phrase_ids = self._replace_mask(full_noun_phrase_ids, masked_ids, 0)
                assert len(embed_ids) == len(removed_noun_phrase_ids) <= self.tokenizer.model_max_length

                # create gold ids
                target_noun_phrase = idx2noun_phrase[target_noun_phrase_idx]
                gold_noun_phrase_ids = [
                    idx for idx in same_noun_phrase_ids[target_noun_phrase]
                    if (idx in exist_noun_phrase_ids) and (idx != target_noun_phrase_idx)
                ]
                if gold_noun_phrase_ids:
                    # the gold idx is assigned to a last token of a noun phrase
                    gold_ids = [int(np.where(removed_noun_phrase_ids == idx)[0][-1]) for idx in gold_noun_phrase_ids]
                else:
                    continue  # don't create instance

                instance = PzeroMaskedInstance(
                    input_ids=embed_ids.tolist(),
                    masked_idx=int(masked_ids[0]),
                    gold_ids=gold_ids,
                )

                yield instance

    @staticmethod
    def _read_parsed_file(file_path: str) -> Generator[List[List[ChunkInstance]], None, None]:
        """text parsed with CaboCha -> a document

        Args:
            * file_path (str): Path to text parsed with CaboCha
        Yields:
            * document (List[List[ChunkInstance]]): The list of chunks corresponding to a document.
        """

        document: List[List[ChunkInstance]] = []
        for chunks in extract_chunk_from_parsed_text(file_path):
            if len(chunks) == 0:
                yield document
                document = []
            else:
                document.append(chunks)

        if len(document) != 0:
            yield document

    @staticmethod
    def _read_chunk_file(file_path: str) -> Generator[List[List[ChunkInstance]], None, None]:
        """jsonl format data -> a document

        Args:
            * file_path (str):
                Path to the jsonl format data generated by
                the function 'preprocess.extract_chunk_from_parsed_text'
        Yields:
            * document (List[List[ChunkInstance]]):
                The list of chunks corresponding to a document.
        """

        loader: Generator[str, None, None] = read_lines(file_path)
        key_func = (lambda x: not bool(x.strip()))

        for is_blank, dumped_lines in tqdm(groupby(loader, key=key_func)):
            if is_blank:
                continue

            document = [json.loads(dumped_line) for dumped_line in dumped_lines]
            yield document

    def _create_noun_phrase_positions(self, chunk: ChunkInstance, assign_number: int = 1) -> Tuple[str, List[int]]:
        """Extract a noun phrase for each chunk

        Args:
            * chunk (ChunkInstance):
                The target chunk to extract a noun phrase (NP)
            * assign_number (int):
                This number is assigned to the position of the tokens that makes up the NP
        Returns:
            * noun_phase (str):
                The noun phrase (NP) in the target chunk
            * noun_phrase_ids (List[int]):
                The indices indicating whether a token makes up a noun phrase or not.
                (assigning 'assign_number' to the tokens corresponding to the NP, otherwise 0)
        """
        len_words = len(chunk['surfs'])
        poss = chunk['poss']
        pos_details = chunk['pos_details']

        if '名詞' not in poss or '動詞' in poss:
            return '', [0] * len_words

        # search NP
        start = 0
        end = len_words - 1
        while (poss[end] != '名詞') and (pos_details[end] != '名詞性名詞接尾辞') and (pos_details[end] != '名詞性特殊接尾辞'):
            end -= 1
        while poss[start] == '特殊':
            start += 1
        if '括弧始' in pos_details[start:end + 1]:
            while pos_details[end] != '括弧始':
                end -= 1
        else:
            end += 1

        noun_phrase_ids = [assign_number] * len_words
        noun_phrase_ids[:start] = [0] * start
        noun_phrase_ids[end:] = [0] * (len_words - end)
        assert len(noun_phrase_ids) == len_words

        noun_phrase = ''.join(''.join(chunk['surfs'][start:end]))

        if not noun_phrase.strip(self.eng_punc_num_chars) or noun_phrase in self.exclude_words:
            return '', [0] * len_words

        return noun_phrase, noun_phrase_ids

    def _reduce_previous_tokens(self, sentences: deque) -> deque:
        """Shave the input sequence to fit the maximum input length of the model

        Args:
            * sentences (collections.deque)
        Returns:
            * sentences (collections.deque): The length of items is less than or equal to 'tokenizer.model_max_length'.
        """
        n_cls_token = 1
        n_tokens = sum(len(tokens) for tokens in sentences)
        n_sep_tokens = len(sentences)

        n_total_tokens = n_cls_token + n_tokens + n_sep_tokens
        model_max_length = self.tokenizer.model_max_length

        while n_total_tokens > model_max_length:
            n_first_sentence_tokens = len(sentences[0])
            modified_n_tokens = n_total_tokens - n_first_sentence_tokens - 1

            # exclude the first sentence (continue)
            if modified_n_tokens >= model_max_length:
                sentences.popleft()
                n_total_tokens = modified_n_tokens
                continue

            # remove the head tokens (break)
            else:
                n_remove_tokens = n_total_tokens - model_max_length
                sentences[0] = sentences[0][n_remove_tokens:]
                break

        return sentences

    @staticmethod
    def _insert_sep(sentences: deque, sep: Any) -> List[Any]:
        """Adding a 'sep' between sentences and aggregating the sentences into a single list

        Args:
            * sentences (deque): deque(sentence, sentence, ...) where 'sentence' is the list of items
            * sep (Any): The item to be inserted between sentences

        Returns:
            * agg_sentences (List[Any]): Aggregated sentences with sep in between sentences
        """

        agg_sentences = []
        for sentence in sentences:
            agg_sentences.extend(sentence)
            agg_sentences.append(sep)

        return agg_sentences

    @staticmethod
    def _replace_mask(input_ids: np.ndarray, replace_ids: np.ndarray, mask_idx: int) -> np.ndarray:
        """Replace a span with a mask

        Args:
            * input_ids (np.ndarray): The target ids to replace a span with a mask
            * replace_ids (np.ndarray): The position to replace with a mask
            * mask_idx (int): The mask idx

        Returns:
            * inserted_ids (np.ndarray): The ids where a span was replaced with a mask
        """
        insert_mask_idx = int(replace_ids[0])
        deleted_ids = np.delete(input_ids, replace_ids)
        inserted_ids = np.insert(deleted_ids, insert_mask_idx, mask_idx)

        return inserted_ids


# For finetuning
def preprocess_ntc_dataset(dir_path: str) -> Generator[NTCPasDocument, None, None]:
    """Preprocessing NAIST Text Corpus (NTC) for ZAR

    Args:
        * dir_path (str):
            Path to the directory containing the documents of NTC.
            The function assumes that the directory is a partitioned data set (train, dev, test).
    Yields:
        * ntc_pas_doc (NTCPasDocument):
            A document parsed for PAS
    """
    # create preprocessed corpus
    logger.info(f'Target Corpus: {dir_path}')
    data_fns = sorted(glob(path.join(dir_path, '*')))
    corpus = create_corpus(data_fns)
    print_stats(corpus)

    # create output file
    for doc in tqdm(corpus, maxinterval=len(corpus)):
        ntc_pas_doc = create_ntc_pas_document(doc)
        yield ntc_pas_doc


def reconfigure_sw2w_dict(sw2w_position: Dict[str, List[int]]) -> Dict[int, Tuple[int, int]]:
    """Reconfigure the key and value types of 'sw2w_position' changed by 'json.dumps'"""
    new_sw2w_position: Dict[int, Tuple[int, int]] = {
        int(key): tuple(value)
        for key, value in sw2w_position.items()
    }
    return new_sw2w_position


# --- AS Model ---
class PreprocessForFinetuningAS(Preprocessor):
    """Preprocessing NAIST Text Corpus (NTC) for AS Model"""

    def __init__(self, is_intra: bool = False, is_processed_path: bool = False) -> None:
        """
        Args:
            * is_intra (bool):
                If true, This class creates instances for 'intra-zero' or 'dep'.
                This created instance consists of a single sentence.
            * is_processed_path (bool):
                If true, 'file_path' in '__call__' is the path to the output file generated
                by 'preprocess.preprocess_ntc_dataset'.
        """
        super().__init__()
        self.is_intra = is_intra
        self.is_processed_path = is_processed_path

        self.n_cases = defaultdict(int)

    def __call__(self, file_path: str) -> Generator[AsTrainingInstance, None, None]:
        """NAIST Text Corpus -> instances for AS model
           or
           jsonl format data generated by 'preprocess.preprocess_ntc_dataset' -> instances for AS model

        Args:
            * file_path (str):
                Path to the directory of NAIST Text Corpus (e.g. 'tests/samples/dummy_ntc')
                or jsonl format data generated by the function 'preprocess.preprocess_ntc_dataset'
                (e.g. 'tests/samples/ntc.instances.jsonl')
        Yields:
            * as_instance (AsTrainingInstance):
                The instance for AS model
        """
        if self.is_processed_path:
            for ntc_doc in self._read_preprocessed_instance(file_path):
                for as_instance in self.create_as_instances(ntc_doc):
                    yield as_instance
        else:
            for ntc_doc in self._read_ntc_file_and_preprocess(file_path):
                for as_instance in self.create_as_instances(ntc_doc):
                    yield as_instance

        self.print_stats()

    def create_as_instances(self, ntc_doc: NTCPasDocument) -> Generator[AsTrainingInstance, None, None]:
        """processed document -> instances for AS model

        Args:
            * ntc_doc (NTCPasDocument): A document generated by 'preprocess.preprocess_ntc_dataset'
        Yields:
            * as_instance (AsTrainingInstance): The instance for AS model
        """

        for pas in ntc_doc['pas_list']:
            max_length = self.tokenizer.model_max_length
            subwords, sw2w_position, w2sw_position = self._convert_word_to_subword(
                ntc_doc['sents'],
                pas['prd_sent_idx'],
                max_length
            )

            # inputs
            input_ids = self.tokenizer.convert_tokens_to_ids(subwords)
            predicate_position_ids = [
                sw_idx
                for w_idx in pas['prd_word_ids']
                for sw_idx in w2sw_position[(pas['prd_sent_idx'], w_idx)]
            ]
            xs_len = len(input_ids)

            # for check bug
            input_tokens = self.tokenizer.convert_ids_to_tokens(input_ids)

            # golds
            gold_positions: AsGoldPositions = dict()
            exo_idx: AsGoldExo = dict()  # -100 is an ID to be ignored

            for case_name, gold_label in pas['gold_labels'].items():
                case_gold_positions, case_exo_idx = self._create_gold_label(gold_label, w2sw_position)
                gold_positions[case_name] = case_gold_positions
                exo_idx[case_name] = case_exo_idx

            assert set(gold_positions) == set(TARGET_CASES)
            assert set(exo_idx) == set(TARGET_CASES)

            # eval_info
            eval_info = PasEvalInfo(
                prd_word_ids=pas['prd_word_ids'],
                prd_sent_idx=pas['prd_sent_idx'],
                file_path=ntc_doc['file_path'],
                sw2w_position=sw2w_position,
            )

            training_instance = AsTrainingInstance(
                input_tokens=input_tokens,
                input_ids=input_ids,
                predicate_position_ids=predicate_position_ids,
                xs_len=xs_len,
                gold_positions=gold_positions,
                exo_idx=exo_idx,
                eval_info=eval_info,
            )

            yield training_instance

    def print_stats(self):
        logger.info('# of instances to use for training')
        for case_name in TARGET_CASES:
            n_case_types = ', '.join(
                f"{case_type}: {self.n_cases[f'{case_type}_{case_name}']:>5}"
                for case_type in CASE_TYPES
            )
            logger.info(f'  - [{case_name}] ' + n_case_types)

        logger.info('# of instances removed')
        for case_name in TARGET_CASES:
            n_case_types = ', '.join(
                f"{case_type}: {self.n_cases[f'_remove_{case_type}_{case_name}']:>5}"
                for case_type in CASE_TYPES
            )
            logger.info(f'  - [{case_name}] ' + n_case_types)

        logger.info('(# of antecedents to use for training)')
        for case_name in TARGET_CASES:
            n_case_types = ', '.join(
                f"{case_type}: {self.n_cases[f'_case_{case_type}_{case_name}']:>5}"
                for case_type in CASE_TYPES
            )
            logger.info(f'  - [{case_name}] ' + n_case_types)

        logger.info('(# of antecedents removed)')
        for case_name in TARGET_CASES:
            n_case_types = ', '.join(
                f"{case_type}: {self.n_cases[f'_case_remove_{case_type}_{case_name}']:>5}"
                for case_type in CASE_TYPES
            )
            logger.info(f'  - [{case_name}] ' + n_case_types)

    def _convert_word_to_subword(
            self,
            sents: List[List[str]],
            prd_sent_idx: int,
            max_length: int,
    ) -> (
            List[str],
            Dict[int, Tuple[int, int]],
            Dict[Tuple[int, int], List[int]]
    ):
        """Convert words into subwords and generate a dictionary that shows the correspondence between them.

        Args:
            * sents (List[List[str]]) : The List of sentences, where the sentence is the list of words.
            * prd_sent_idx (int) : The index of the sentence containing the target predicate.
            * max_length (int) : The max length of input sequence

        Returns:
            * subwords (List[str]) : The list of subwords as input
            * sw2w_position (Dict[int, Tuple[int, int])        : Convert the position of a subword to that of a word.
            * w2sw_position (Dict[Tuple[int, int], List[int]]) : Convert the position of a word to that of subwords.
        """

        reversed_subwords: List[str] = []
        reversed_sw2w_position: Dict[int, Tuple[int, int]] = {}
        reversed_w2sw_position: Dict[Tuple[int, int], List[int]] = defaultdict(list)

        n_sent = 0
        is_over_length = False
        sent_ids = list(range(len(sents)))

        for sent_idx, sent in zip(reversed(sent_ids[:prd_sent_idx + 1]), reversed(sents[:prd_sent_idx + 1])):
            n_sent += 1
            len_sent = len(sent)
            reversed_subwords.append(self.tokenizer.sep_token)  # (1) add a separation token to the list

            for word_idx, word in enumerate(reversed(sent), 1):
                sws = self._subwords(word)

                # over the maximum length
                if len(reversed_subwords) + len(sws) + 1 > max_length:
                    is_over_length = True
                    break

                word_idx = len_sent - word_idx  # modify word_idx

                for subword in reversed(sws):
                    len_all_subwords = len(reversed_subwords)
                    reversed_sw2w_position[len_all_subwords] = (sent_idx, word_idx)
                    reversed_w2sw_position[(sent_idx, word_idx)].append(len_all_subwords)
                    reversed_subwords.append(subword)  # (2) add a subword to the list

            # over the maximum length
            if is_over_length:
                break

            # intra
            if self.is_intra:
                break

        # modify
        if reversed_subwords[-1] == self.tokenizer.sep_token:
            n_sent -= 1
            reversed_subwords.pop()

        reversed_subwords.append(self.tokenizer.cls_token)  # (3) add a cls token to the list

        # reverse
        base_idx = len(reversed_subwords) - 1
        subwords: List[str] = list(reversed(reversed_subwords))
        sw2w_position: Dict[int, Tuple[int, int]] = {
            base_idx - sw_idx: (sent_idx, word_idx)
            for sw_idx, (sent_idx, word_idx) in reversed_sw2w_position.items()
        }
        w2sw_position: Dict[Tuple[int, int], List[int]] = {
            (sent_idx, word_idx): list(reversed([base_idx - sw_idx for sw_idx in sw_ids]))
            for (sent_idx, word_idx), sw_ids in reversed_w2sw_position.items()
        }

        return subwords, sw2w_position, w2sw_position

    def _create_gold_label(
            self,
            pas_gold_label: PasGoldLabel,
            w2sw_position: Dict[Tuple[int, int], List[int]],
    ) -> (
            List[int],
            int
    ):
        """Create gold label

        Args:
            * pas_gold_label (PasGoldLabel) : PAS gold label for each case
            * w2sw_position (Dict[Tuple[int, int], List[int]]) : Convert the position of a word to that of subwords.
        Returns:
            * gold_positions (List[int]) : The position of gold labels in 'input_ids'
            * exo_idx (int) : Exophoric gold label (details in 'instance.AsGoldExo')
        """

        gold_positions: List[int] = []
        exo_idx: int = -100

        case_name = pas_gold_label['case_name']
        case_type = pas_gold_label['case_type']

        if case_type in NO_ANSWERS:
            gold_positions.append(0)
            exo_idx = EXO_INDEX[case_type]
            self.n_cases[f'{case_type}_{case_name}'] += 1
        else:
            for gold_case in pas_gold_label['gold_cases']:
                # Append the position of the subword to the list if the answer is included in the input_ids
                sw_ids = w2sw_position.get((gold_case['sent_idx'], gold_case['word_idx']))
                if sw_ids is not None:
                    assert len(sw_ids) >= 1
                    gold_positions.append(sw_ids[-1])
                    self.n_cases[f"_case_{gold_case['case_type']}_{case_name}"] += 1
                else:
                    self.n_cases[f"_case_remove_{gold_case['case_type']}_{case_name}"] += 1

            if len(gold_positions) != 0:
                self.n_cases[f'{case_type}_{case_name}'] += 1
            else:
                gold_positions.append(0)
                self.n_cases[f'_remove_{case_type}_{case_name}'] += 1

        return gold_positions, exo_idx

    @staticmethod
    def _read_ntc_file_and_preprocess(dir_path: str) -> Generator[NTCPasDocument, None, None]:
        """NTC -> NTCPasDocument

        Args:
            * dir_path (str):
                Path to the directory containing the documents of NTC.
                The function assumes that the directory is a partitioned data set (train, dev, test).
        Yields:
            * ntc_pas_doc (NTCPasDocument):
                A document parsed for PAS
        """
        for instance in preprocess_ntc_dataset(dir_path):
            yield instance

    @staticmethod
    def _read_preprocessed_instance(file_path: str) -> Generator[NTCPasDocument, None, None]:
        """jsonl format data -> NTCPasDocument

        Args:
            * file_path (str):
                Path to the jsonl format data generated by
                the function 'preprocess.preprocess_ntc_dataset'
        Yields:
            * ntc_pas_doc (NTCPasDocument):
                A document parsed for PAS
        """
        for line in read_lines(file_path):
            instance = json.loads(line)
            yield instance


# --- AS-Pzero Model ---
class PreprocessForFinetuningASPzero(PreprocessForFinetuningAS):
    def __init__(self, is_intra: bool = False, is_processed_path: bool = False) -> None:
        """
        Args:
            * is_intra (bool):
                If true, This class creates instances for 'intra-zero' or 'dep'.
                This created instance consists of a single sentence.
            * is_processed_path (bool):
                If true, 'file_path' in '__call__' is the path to the output file generated
                by 'preprocess.preprocess_ntc_dataset'.
        """
        super().__init__(is_intra=is_intra, is_processed_path=is_processed_path)

    def __call__(self, file_path: str) -> Generator[AsPzeroTrainingInstance, None, None]:
        """NAIST Text Corpus -> instances for AS-Pzero model
           or
           jsonl format data generated by 'preprocess.preprocess_ntc_dataset' -> instances for AS model

        Args:
            * file_path (str):
                Path to the directory of NAIST Text Corpus (e.g. 'tests/samples/dummy_ntc')
                or jsonl format data generated by the function 'preprocess.preprocess_ntc_dataset'
                (e.g. 'tests/samples/ntc.instances.jsonl')
        Yields:
            * as_pzero_instance (AsPzeroTrainingInstance):
                The instance for AS-Pzero model
        """
        if self.is_processed_path:
            for ntc_doc in self._read_preprocessed_instance(file_path):
                for as_pzero_instance in self.create_as_pzero_instances(ntc_doc):
                    yield as_pzero_instance
        else:
            for ntc_doc in self._read_ntc_file_and_preprocess(file_path):
                for as_pzero_instance in self.create_as_pzero_instances(ntc_doc):
                    yield as_pzero_instance

        self.print_stats()

    def create_as_pzero_instances(self, ntc_doc: NTCPasDocument) -> Generator[AsPzeroTrainingInstance, None, None]:
        """processed document -> instances for AS-Pzero model

        Args:
            * ntc_doc (NTCPasDocument): A document generated by 'preprocess.preprocess_ntc_dataset'
        Yields:
            * as_pzero_instance (AsPzeroTrainingInstance): the instance for AS-Pzero model
        """

        for pas in ntc_doc['pas_list']:

            len_prd_tokens = self._len_predicate_tokens(ntc_doc['sents'], pas)
            max_length = self.tokenizer.model_max_length - len_prd_tokens - 2
            subwords, sw2w_position, w2sw_position = self._convert_word_to_subword(
                ntc_doc['sents'],
                pas['prd_sent_idx'],
                max_length,
            )

            # inputs
            predicate_position_ids = [
                sw_idx
                for w_idx in pas['prd_word_ids']
                for sw_idx in w2sw_position[(pas['prd_sent_idx'], w_idx)]
            ]
            base_input_ids = self.tokenizer.convert_tokens_to_ids(subwords)
            mask_position_id = len(base_input_ids)
            base_input_ids.append(self.tokenizer.mask_token_id)  # (1) insert a mask token

            for case_name, gold_label in pas['gold_labels'].items():
                # inputs
                input_ids = deepcopy(base_input_ids)
                input_ids.append(self.tokenizer.vocab[CASE_SURFACES[case_name]])  # (2) insert a target argument label
                input_ids += [input_ids[idx] for idx in predicate_position_ids]  # (3) insert a target predicate
                xs_len = len(input_ids)

                # for check bug
                input_tokens = self.tokenizer.convert_ids_to_tokens(input_ids)

                # golds
                gold_positions, exo_idx = self._create_gold_label(gold_label, w2sw_position)

                # eval_info
                eval_info = PasEvalInfo(
                    prd_word_ids=pas['prd_word_ids'],
                    prd_sent_idx=pas['prd_sent_idx'],
                    file_path=ntc_doc['file_path'],
                    sw2w_position=sw2w_position,
                )

                training_instance = AsPzeroTrainingInstance(
                    input_tokens=input_tokens,
                    input_ids=input_ids,
                    predicate_position_ids=predicate_position_ids,
                    mask_position_id=mask_position_id,
                    xs_len=xs_len,
                    gold_positions=gold_positions,
                    exo_idx=exo_idx,
                    eval_info=eval_info,
                    case_name=case_name,
                )

                yield training_instance

    def _len_predicate_tokens(self, sents: List[List[str]], pas: Pas) -> int:
        """Compute the length of tokens making up a target predicate

        Args:
            * sents (List[List[str]]) : The list of sentences, where a sentence is the list of words
            * pas (Pas) : The target predicate-argument structure
        Return:
            * len_predicate_tokens : The length of tokens making up a target predicate
        """

        prd_word_ids = pas['prd_word_ids']
        prd_sent_idx = pas['prd_sent_idx']
        len_predicate_tokens = sum([len(self._subwords(sents[prd_sent_idx][w_idx])) for w_idx in prd_word_ids])

        return len_predicate_tokens


def create_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description='Preprocessing')
    parser.add_argument('--in', dest='in_file', type=path.abspath, required=True,
                        help='Path to input file (or directory).')
    parser.add_argument('--type', type=str, choices=['raw', 'chunk', 'cloze', 'pzero', 'ntc', 'as', 'as-pzero'],
                        help='The type of preprocessing')

    return parser


def main():
    parser = create_parser()
    args = parser.parse_args()
    logger.info(args)

    logger.info(f'Type: {args.type}')
    if args.type == 'raw':
        preprocessor = convert_han_to_zen

    elif args.type == 'chunk':
        preprocessor = extract_chunk_from_parsed_text

    elif args.type == 'cloze':
        preprocessor = PreprocessForPretrainingCloze()

    elif args.type == 'ntc':
        preprocessor = preprocess_ntc_dataset

    elif args.type == 'pzero':
        preprocessor = PreprocessForPretrainingPzero()

    elif args.type == 'as':
        preprocessor = PreprocessForFinetuningAS()

    elif args.type == 'as-pzero':
        preprocessor = PreprocessForFinetuningASPzero()

    else:
        raise ValueError(f'Unsupported value: {args.type}')

    for instance in preprocessor(args.in_file):
        if len(instance) == 0:
            print()
        elif args.type == 'raw':
            print(instance)
        else:
            print(json.dumps(instance, ensure_ascii=False))


if __name__ == '__main__':
    main()
